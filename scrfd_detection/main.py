# main.py ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è

import cv2
import time
import numpy as np
import threading
from queue import Queue, Full, Empty
from detection import FaceDetector
from recognizer import FaceRecognizer
from camera import AsyncCameraReader
import sys

from hardware_detection import estimate_hardware_level, get_optimal_settings, HardwareLevel, select_hardware_level_interactive

# –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
from PIL import Image, ImageDraw, ImageFont


def align_face_by_kps(frame, kps, output_size=(112, 112)):
    ref = np.array(
        [
            [38.2946, 51.6963],
            [73.5318, 51.5014],
            [56.0252, 71.7366],
            [41.5493, 92.3655],
            [70.7299, 92.2041],
        ],
        dtype=np.float32,
    )
    src = kps.astype(np.float32)
    transform = cv2.estimateAffinePartial2D(src, ref, method=cv2.LMEDS)[0]
    if transform is None:
        return None
    aligned = cv2.warpAffine(frame, transform, output_size, flags=cv2.INTER_LINEAR)
    return aligned


class GlobalFaceTrack:
    def __init__(self, person_id: str):
        self.person_id = person_id
        self.last_seen = time.time()
        self.last_saved_ts = 0.0
        self.last_centers = {}

    def update_center(self, camera_index: int, bbox_center):
        self.last_centers[camera_index] = bbox_center
        self.last_seen = time.time()


class GlobalFaceSaver:
    def __init__(self, recognizer: FaceRecognizer, save_interval_sec: float = 2.0):
        self.recognizer = recognizer
        self.save_interval = save_interval_sec
        self.tracks = []

    @staticmethod
    def _center_of_bbox(bbox):
        x1, y1, x2, y2 = bbox
        return ((x1 + x2) // 2, (y1 + y2) // 2)

    @staticmethod
    def _distance(c1, c2):
        if c1 is None or c2 is None:
            return float('inf')
        dx = c1[0] - c2[0]
        dy = c1[1] - c2[1]
        return (dx*dx + dy*dy)**0.5

    def _find_track_by_id(self, person_id: str) -> GlobalFaceTrack:
        for tr in self.tracks:
            if tr.person_id == person_id:
                return tr
        return None

    def _find_track_by_proximity(self, camera_index: int, bbox_center, max_dist=100) -> GlobalFaceTrack:
        for tr in self.tracks:
            if camera_index in tr.last_centers:
                if self._distance(tr.last_centers[camera_index], bbox_center) <= max_dist:
                    return tr
        return None

    def _get_or_create_track(self, camera_index: int, bbox_center, person_id_hint: str) -> GlobalFaceTrack:
        if person_id_hint != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
            tr = self._find_track_by_id(person_id_hint)
            if tr:
                tr.update_center(camera_index, bbox_center)
                return tr

        tr = self._find_track_by_proximity(camera_index, bbox_center)
        if tr:
            if tr.person_id == "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ" and person_id_hint != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
                tr.person_id = person_id_hint
                print(f"üîó –û–±–Ω–æ–≤–ª—ë–Ω ID: —Ç–µ–ø–µ—Ä—å {person_id_hint}")
            tr.update_center(camera_index, bbox_center)
            return tr

        if person_id_hint == "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
            new_id = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        else:
            new_id = person_id_hint

        tr = GlobalFaceTrack(new_id)
        tr.update_center(camera_index, bbox_center)
        self.tracks.append(tr)
        print(f"üÜï –°–æ–∑–¥–∞–Ω —Ç—Ä–µ–∫ –¥–ª—è {new_id} —Å –∫–∞–º–µ—Ä—ã {camera_index}")
        return tr

    def maybe_save(self, camera_index: int, bbox, face_img_bgr, person_id_hint: str, similarity: float):
        if person_id_hint == "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
            return None
        if similarity >= 0.9:
            return None

        now = time.time()
        center = self._center_of_bbox(bbox)
        tr = self._get_or_create_track(camera_index, center, person_id_hint)

        if now - tr.last_saved_ts < self.save_interval:
            return None

        saved_path = self.recognizer.add_image_to_person(tr.person_id, face_img_bgr)
        tr.last_saved_ts = now
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è {tr.person_id} (–∫–∞–º–µ—Ä–∞ {camera_index})")
        return saved_path


class AsyncFaceProcessor:
    def __init__(self, camera_index: int, detector, recognizer, saver, max_queue_size=3, process_interval=4.0):
        self.camera_index = camera_index
        self.detector = detector
        self.recognizer = recognizer
        self.saver = saver
        self.frame_queue = Queue(maxsize=max_queue_size)
        self.result_queue = Queue(maxsize=1)
        self.process_interval = process_interval

        height = 480
        width = 640
        blank = np.zeros((height, width, 3), dtype=np.uint8)
        blank = put_text_russian(blank, f"–ò—Å—Ç–æ—á–Ω–∏–∫ {camera_index + 1} –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø...", (50, height//2),
                                font_path=get_font_path(), font_size=24, color=(255, 255, 0))

        self.last_result = {
            'frame': blank,
            'faces': [],
            'camera_index': camera_index,
            'original_size': (width, height)
        }

        self.stop_event = threading.Event()
        self.thread = None

    def start(self):
        self.thread = threading.Thread(target=self.run, daemon=True)
        self.thread.start()
        print(f"üß† –ü–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∫–∞–º–µ—Ä—ã {self.camera_index} –∑–∞–ø—É—â–µ–Ω")

    def run(self):
        print(f"‚ñ∂Ô∏è  [Processor {self.camera_index}] –ó–∞–ø—É—â–µ–Ω")
        last_process_time = 0
        min_interval = self.process_interval
        frame_count = 0
        last_fps_time = time.time()
        processed_count = 0

        while not self.stop_event.is_set():
            try:
                frame = self.frame_queue.get_nowait()
                frame_count += 1
            except Empty:
                time.sleep(0.01)
                continue

            now = time.time()
            if now - last_process_time < min_interval:
                continue

            last_process_time = now
            processed_count += 1

            if processed_count % 5 == 0:
                elapsed = now - last_fps_time
                avg_fps = 5 / elapsed if elapsed > 0 else 0
                print(f"üìà [Processor {self.camera_index}] –°—Ä–µ–¥–Ω–∏–π FPS –æ–±—Ä–∞–±–æ—Ç–∫–∏: {avg_fps:.2f} (–≤—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count})")
                last_fps_time = now

            print(f"üß† [Processor {self.camera_index}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä #{frame_count} (–∏–Ω—Ç–µ—Ä–≤–∞–ª: {min_interval} —Å–µ–∫)")

            try:
                faces = self.detector.detect(frame)
                face_patches = []
                face_bboxes = []

                for face in faces:
                    bbox = face.bbox.astype(int)
                    x1, y1, x2, y2 = bbox
                    raw_face = frame[y1:y2, x1:x2]
                    if raw_face.size == 0:
                        continue

                    patch = raw_face
                    if hasattr(face, 'kps') and face.kps is not None and np.array(face.kps).shape == (5, 2):
                        aligned = align_face_by_kps(frame, face.kps)
                        if aligned is not None:
                            patch = aligned

                    face_patches.append(patch)
                    face_bboxes.append((x1, y1, x2, y2))

                names = []
                sims = []
                if face_patches:
                    names, sims = self.recognizer.recognize_batch(face_patches)

                current_faces = []
                for i, (x1, y1, x2, y2) in enumerate(face_bboxes):
                    face_img = face_patches[i]
                    name = names[i]
                    sim = sims[i]

                    current_faces.append({
                        'bbox': (x1, y1, x2, y2),
                        'name': name,
                        'sim': sim,
                        'face_img': face_img.copy(),
                    })

                    color = (0, 255, 0) if name != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ" else (0, 0, 255)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                    label = f"{name} ({sim:.2f})"
                    if name == "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
                        label += " ‚Äî –∫–ª–∏–∫–Ω–∏—Ç–µ"

                    # ‚úÖ –°–¥–≤–∏–Ω—É–ª–∏ –í–´–®–ï –∏ —Å–¥–µ–ª–∞–ª–∏ –ö–†–£–ü–ù–ï–ï
                    frame = put_text_russian(frame, label, (x1, y1 - 30),
                                             font_path=get_font_path(), font_size=26, color=color)

                    saved_path = self.saver.maybe_save(self.camera_index, (x1, y1, x2, y2), face_img, name, sim)
                    if saved_path:
                        # ‚úÖ –ö—Ä—É–ø–Ω–µ–µ
                        frame = put_text_russian(frame, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ", (x1, y2 + 20),
                                                 font_path=get_font_path(), font_size=22, color=(255, 255, 0))

                    if hasattr(face, 'kps'):
                        kps = face.kps.astype(int)
                        for pt in kps:
                            cv2.circle(frame, tuple(pt), 3, (255, 0, 0), -1)

                # ‚úÖ –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                # ‚úÖ –ö—Ä—É–ø–Ω–µ–µ
                frame = put_text_russian(frame, f"–ò—Å—Ç–æ—á–Ω–∏–∫ {self.camera_index + 1}", (10, 70),
                                         font_path=get_font_path(), font_size=28, color=(255, 255, 255))

                result = {
                    'frame': frame,
                    'faces': current_faces,
                    'camera_index': self.camera_index,
                    'original_size': (frame.shape[1], frame.shape[0])
                }
                self.last_result = result

                try:
                    if self.result_queue.full():
                        self.result_queue.get_nowait()
                    self.result_queue.put_nowait(result)
                except Full:
                    pass

                print(f"‚úÖ [Processor {self.camera_index}] –ö–∞–¥—Ä #{frame_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")

            except Exception as e:
                print(f"‚ùå [Processor {self.camera_index}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                continue

    def stop(self):
        self.stop_event.set()
        if self.thread:
            self.thread.join(timeout=2.0)

    def submit_frame(self, frame):
        try:
            if self.frame_queue.full():
                self.frame_queue.get_nowait()
            self.frame_queue.put_nowait(frame)
        except (Full, Empty):
            pass

    def get_result(self, timeout=0.01):
        try:
            result = self.result_queue.get(timeout=timeout)
            self.last_result = result
            return result
        except Empty:
            return self.last_result


def find_available_cameras(max_tested=20):
    available = []
    for i in range(max_tested):
        cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)
        if not cap.isOpened():
            cap.release()
            continue

        for _ in range(3):
            ret, _ = cap.read()
            if ret:
                available.append(i)
                break
            time.sleep(0.1)
        cap.release()

    return available


# ‚úÖ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
def get_font_path():
    candidates = [
        "arial.ttf",
        "C:/Windows/Fonts/arial.ttf",
        "DejaVuSans.ttf",
        "times.ttf",
        "C:/Windows/Fonts/times.ttf",
        "verdana.ttf",
        "C:/Windows/Fonts/verdana.ttf"
    ]
    for path in candidates:
        try:
            ImageFont.truetype(path, 10)
            return path
        except:
            continue
    return "arial.ttf"  # fallback


def put_text_russian(img, text, org, font_path="arial.ttf", font_size=24, color=(255, 255, 255)):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)

    try:
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        font = ImageFont.load_default()
        print(f"‚ö†Ô∏è  –®—Ä–∏—Ñ—Ç {font_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π.")

    draw.text(org, text, font=font, fill=color[::-1])  # PIL: RGB, OpenCV: BGR

    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


def create_camera_grid(frames, min_h):
    n = len(frames)
    if n == 0:
        return np.zeros((min_h, 640, 3), dtype=np.uint8)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ç–∫—É
    if n == 1:
        rows, cols = 1, 1
    elif n == 2:
        rows, cols = 1, 2
    elif n == 3:
        rows, cols = 2, 2
    elif n <= 4:
        rows, cols = 2, 2
    elif n <= 6:
        rows, cols = 2, 3
    elif n <= 9:
        rows, cols = 3, 3
    else:
        import math
        cols = math.ceil(math.sqrt(n))
        rows = math.ceil(n / cols)

    resized_frames = []
    max_widths = []
    for frame in frames:
        scale = min_h / frame.shape[0]
        new_w = int(frame.shape[1] * scale)
        resized = cv2.resize(frame, (new_w, min_h))
        resized_frames.append(resized)
        max_widths.append(new_w)

    grid_h = rows * min_h
    grid_w = cols * max(max_widths) if max_widths else 640
    grid = np.zeros((grid_h, grid_w, 3), dtype=np.uint8)

    for i, frame in enumerate(resized_frames):
        row = i // cols
        col = i % cols

        x_offset = col * max(max_widths) if max_widths else 0
        y_offset = row * min_h

        h, w = frame.shape[:2]
        x_centered = x_offset + (max(max_widths) - w) // 2 if max_widths else x_offset
        grid[y_offset:y_offset+h, x_centered:x_centered+w] = frame

    return grid


def main():
    estimated_level, score, details = estimate_hardware_level()
    selected_level = select_hardware_level_interactive(estimated_level)
    settings = get_optimal_settings(selected_level)

    print("\nüöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"   üíª –£—Ä–æ–≤–µ–Ω—å: {selected_level.upper()}")
    print(f"   ‚öôÔ∏è  –û–±—Ä–∞–±–æ—Ç–∫–∞: 1 –∫–∞–¥—Ä / {settings['process_interval_sec']} —Å–µ–∫")
    print(f"   üì∑ –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {settings['camera_width']}x{settings['camera_height']}")
    print(f"   üéûÔ∏è  FPS –∫–∞–º–µ—Ä: {settings['camera_fps']}")
    print(f"   üß© –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∏—Ü: –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω–æ ‚úÖ")
    print("   üì∏ –ö–∞–º–µ—Ä—ã: –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∏–¥–µ–æ–∏—Å—Ç–æ—á–Ω–∏–∫–∏\n")

    auto_clear = False
    try:
        ans = input("–û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –ª–∏—Ü –∏ –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º? (y/N): ").strip().lower()
        auto_clear = ans in ("y", "yes", "–¥", "–¥–∞")
    except Exception:
        pass

    detector = FaceDetector(model_name='scrfd_10g_kps', device_id=0)
    recognizer = FaceRecognizer(force_rebuild=auto_clear)
    if auto_clear:
        recognizer.clear_database_and_cache()
    saver = GlobalFaceSaver(recognizer, save_interval_sec=2.0)

    print("üîç –ü–æ–∏—Å–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–∞–º–µ—Ä...")
    camera_indices = find_available_cameras(max_tested=20)
    print(f"üé• –ù–∞–π–¥–µ–Ω—ã –∫–∞–º–µ—Ä—ã: {camera_indices}")

    if not camera_indices:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ–∏—Å—Ç–æ—á–Ω–∏–∫–∞")
        return

    camera_readers = []
    processors = []

    for idx in camera_indices:
        reader = AsyncCameraReader(
            idx,
            width=settings['camera_width'],
            height=settings['camera_height'],
            fps=settings['camera_fps']
        )
        if reader.start():
            camera_readers.append(reader)
            warmup_time = 0.5
            print(f"‚è≥ [–ö–∞–º–µ—Ä–∞ {idx}] –ü—Ä–æ–≥—Ä–µ–≤ –Ω–∞ {warmup_time} —Å–µ–∫...")
            time.sleep(warmup_time)

            processor = AsyncFaceProcessor(
                idx,
                detector,
                recognizer,
                saver,
                process_interval=settings['process_interval_sec']
            )
            processor.start()
            processors.append(processor)
        else:
            print(f"‚ö†Ô∏è  –ö–∞–º–µ—Ä–∞ {idx} –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞")

    if not processors:
        print("‚ùå –ù–∏ –æ–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω")
        return

    print("üîç –¢–µ—Å—Ç: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ –∫–∞–¥—Ä–æ–≤ –Ω–∞–ø—Ä—è–º—É—é...")
    for reader in camera_readers:
        for i in range(3):
            frame = reader.get_frame(timeout=1.0)
            if frame is not None:
                print(f"‚úÖ [–¢–µ—Å—Ç] –ö–∞–º–µ—Ä–∞ {reader.camera_index}: –ø–æ–ª—É—á–µ–Ω –∫–∞–¥—Ä {frame.shape}")
                break
            else:
                print(f"‚ö†Ô∏è  [–¢–µ—Å—Ç] –ö–∞–º–µ—Ä–∞ {reader.camera_index}: –ø–æ–ø—ã—Ç–∫–∞ {i+1} ‚Äî –∫–∞–¥—Ä None")

    WINDOW_NAME = "–°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü"
    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)

    status_text = ""
    status_until = 0
    current_faces_per_cam = {}

    def on_mouse(event, x, y, flags, userdata=None):
        nonlocal status_text, status_until
        if event == cv2.EVENT_LBUTTONDOWN:
            print(f"üñ±Ô∏è  –ö–ª–∏–∫ –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º: ({x}, {y})")
            for cam_idx, data in current_faces_per_cam.items():
                faces = data['faces']
                offset_x = data['offset_x']
                offset_y = data['offset_y']
                for item in faces:
                    (bx1, by1, bx2, by2) = item['bbox']
                    bx1_global = bx1 + offset_x
                    by1_global = by1 + offset_y
                    bx2_global = bx2 + offset_x
                    by2_global = by2 + offset_y

                    if bx1_global <= x <= bx2_global and by1_global <= y <= by2_global:
                        if item['name'] == "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ":
                            new_id = recognizer.get_next_person_id()
                            recognizer.add_image_to_person(new_id, item['face_img'])
                            status_text = f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω ID {new_id} —Å –∫–∞–º–µ—Ä—ã {cam_idx}"
                            status_until = time.time() + 2.0
                            print(f"üéâ –£–°–ü–ï–•: –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —á–µ–ª–æ–≤–µ–∫ —Å ID {new_id} —Å –∫–∞–º–µ—Ä—ã {cam_idx}")
                        else:
                            status_text = f"‚ÑπÔ∏è –£–∂–µ –≤ –±–∞–∑–µ: {item['name']}"
                            status_until = time.time() + 1.5
                            print(f"üìå –õ–∏—Ü–æ —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–æ: {item['name']}")
                        return

    cv2.setMouseCallback(WINDOW_NAME, on_mouse)
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∑–∞–ø—É—â–µ–Ω–∞. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞. –ö–ª–∏–∫–Ω–∏—Ç–µ –ø–æ —Ä–∞–º–∫–µ '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ' —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –≤ –±–∞–∑—É.")

    try:
        while True:
            frames = []
            current_faces_per_cam.clear()

            for processor in processors:
                result = processor.get_result()
                if result is None:
                    width, height = settings['camera_width'], settings['camera_height']
                    frame = np.zeros((height, width, 3), dtype=np.uint8)
                    frame = put_text_russian(frame, f"–ò—Å—Ç–æ—á–Ω–∏–∫ {processor.camera_index + 1} –û–ñ–ò–î–ê–ù–ò–ï", (50, height//2),
                                           font_path=get_font_path(), font_size=24, color=(0, 255, 255))
                    orig_w, orig_h = width, height
                else:
                    frame = result['frame']
                    orig_w, orig_h = result['original_size']
                frames.append(frame)

            if len(frames) == 0:
                min_h = settings['camera_height']
            else:
                min_h = min(f.shape[0] for f in frames)

            combined = create_camera_grid(frames, min_h)

            # –ü–µ—Ä–µ—Å—á—ë—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–ª—è –∫–ª–∏–∫–∞
            if len(frames) > 0:
                n = len(frames)
                if n == 1:
                    rows, cols = 1, 1
                elif n == 2:
                    rows, cols = 1, 2
                elif n == 3:
                    rows, cols = 2, 2
                elif n <= 4:
                    rows, cols = 2, 2
                elif n <= 6:
                    rows, cols = 2, 3
                elif n <= 9:
                    rows, cols = 3, 3
                else:
                    import math
                    cols = math.ceil(math.sqrt(n))
                    rows = math.ceil(n / cols)

                max_widths = []
                for frame in frames:
                    scale = min_h / frame.shape[0]
                    new_w = int(frame.shape[1] * scale)
                    max_widths.append(new_w)

                max_w = max(max_widths) if max_widths else 640

                for i, processor in enumerate(processors):
                    result = processor.get_result()
                    if result is None:
                        current_faces_per_cam[processor.camera_index] = {
                            'faces': [],
                            'offset_x': 0,
                            'offset_y': 0
                        }
                    else:
                        row = i // cols
                        col = i % cols
                        x_offset = col * max_w
                        y_offset = row * min_h

                        frame = result['frame']
                        scale = min_h / frame.shape[0]
                        w_scaled = int(frame.shape[1] * scale)
                        x_centered = x_offset + (max_w - w_scaled) // 2

                        scaled_faces = []
                        for face in result['faces']:
                            x1, y1, x2, y2 = face['bbox']
                            x1_scaled = int(x1 * scale)
                            y1_scaled = int(y1 * scale)
                            x2_scaled = int(x2 * scale)
                            y2_scaled = int(y2 * scale)
                            scaled_faces.append({
                                'bbox': (x1_scaled, y1_scaled, x2_scaled, y2_scaled),
                                'name': face['name'],
                                'sim': face['sim'],
                                'face_img': face['face_img']
                            })

                        current_faces_per_cam[processor.camera_index] = {
                            'faces': scaled_faces,
                            'offset_x': x_centered,
                            'offset_y': y_offset
                        }

                    reader = next((r for r in camera_readers if r.camera_index == processor.camera_index), None)
                    if reader:
                        frame_raw = reader.get_frame(timeout=0.001)
                        if frame_raw is not None:
                            processor.submit_frame(frame_raw)

            # ‚úÖ –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
            unique_ids = set()
            total_faces = 0
            for data in current_faces_per_cam.values():
                for face in data['faces']:
                    unique_ids.add(face['name'])
                    total_faces += 1

            # ‚úÖ –ö—Ä—É–ø–Ω–µ–µ
            combined = put_text_russian(combined, f'–õ–∏—Ü–∞: {total_faces} | –õ—é–¥–∏: {len(unique_ids)}', (10, 40),
                                        font_path=get_font_path(), font_size=32, color=(0, 0, 255))

            if status_text and time.time() < status_until:
                # ‚úÖ –ö—Ä—É–ø–Ω–µ–µ
                combined = put_text_russian(combined, status_text, (10, 110),
                                            font_path=get_font_path(), font_size=28, color=(0, 255, 255))

            cv2.imshow(WINDOW_NAME, combined)
            cv2.resizeWindow(WINDOW_NAME, combined.shape[1], combined.shape[0])

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break

    except KeyboardInterrupt:
        print("\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è.")

    finally:
        for processor in processors:
            processor.stop()
        for reader in camera_readers:
            reader.stop()
        cv2.destroyAllWindows()
        print("üëã –í—Å–µ –ø–æ—Ç–æ–∫–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –í—ã—Ö–æ–¥.")


if __name__ == '__main__':
    main()